# 迁移学习

---


如今，深度学习领域一个常见的误解在于没有谷歌那样的巨量数据，你将没有希望创建一个有效的深度学习模型。尽管数据是创建网络中至关重要的部分，迁移学习的思路将帮助我们降低数据需求。迁移学习指的是利用预训练模型（神经网络的权重和参数都已经被其他人利用更大规模的数据集训练好了）并用自己的数据集将模型「微调」的过程。这种思路中预训练模型扮演着特征提取器的角色。你将移除网络的最后一层并用你自有的分类器置换（取决于你的问题空间）。然后冻结其他所有层的权重并正常训练该网络（冻结这些层意味着在梯度下降/最优化过程中保持权值不变）。



让我们探讨一下为什么做这项工作。比如说我们正在讨论的这个预训练模型是在 ImageNet （一个包含一千多个分类，一千四百万张图像的数据集）上训练的 。当我们思考神经网络的较低层时，我们知道它们将检测类似曲线和边缘这样的特征。现在，除非你有一个极为独特的问题空间和数据集，你的神经网络也会检测曲线和边缘这些特征。相比通过随机初始化权重训练整个网络，我们可以利用预训练模型的权重（并冻结）聚焦于更重要的层（更高层）进行训练。如果你的数据集不同于 ImageNet 这样的数据集，你必须训练更多的层级而只冻结一些低层的网络。



* Yoshua Bengio （另外一个深度学习先驱 ）论文：How transferable are features in deep neural networks?

* Ali Sharif Razavian 论文：CNN Features off-the-shelf: an Astounding Baseline for Recognition 

* Jeff Donahue 论文：DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition 